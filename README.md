# Building_CNN-
In this part, you will build and evaluate a custom CNN model for image classification. You will work with a multi-class dataset to practice key aspects of building, training, and evaluating CNNs in PyTorch. Expected accuracy on the test dataset: > 85%.
This repository contains the notebook **`notebooks/Building a CNN.ipynb`**, which trains a Convolutional Neural Network for image classification using **PyTorch + torchvision**.

## âœ¨ Overview
- **Dataset:** Custom dataset via ImageFolder
- **Input size:** 28Ã—28 with 1 channel(s)
- **Conv layers:** 6 Â· **Dense layers:** 4
- **Optimizer:** SGD (momentum) Â· **LR:** 0.001 Â· **Batch size:** 64 Â· **Epochs:** 10
- **LR Scheduler:** StepLR
- **Loss:** CrossEntropyLoss
- **Monitoring:** - Confusion matrix & classification report
- **Result:** **Accuracy:** ~92.06%

> The notebook includes data loading, augmentations/transforms, model definition, training loop, evaluation, and plots.

---

## ğŸ“¦ Data

If you use a builtin dataset (e.g., **Custom dataset via ImageFolder**), it will be downloaded automatically.  
For a **custom dataset** with `ImageFolder`, use this layout:

```
data/
â””â”€â”€ train/
    â”œâ”€â”€ class_0/
    â””â”€â”€ class_1/
â””â”€â”€ val/
    â”œâ”€â”€ class_0/
    â””â”€â”€ class_1/
```

Update paths in the notebook where the dataset is created.

---

## ğŸ› ï¸ Setup

### 1) Environment
```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install --upgrade pip
pip install -r requirements.txt
```

### 2) Run
```bash
jupyter notebook notebooks/Building\ a\ CNN.ipynb
```
Run cells in order. If GPU is available, PyTorch/TensorFlow will auto-detect it.

---





---

## âš™ï¸ Reproducibility / Tips
- Set random seeds for `torch`/`tensorflow`, `numpy`, and loader workers.
- Lower `batch_size` if you hit OOM; consider mixed precision (`torch.cuda.amp` or `tf.keras.mixed_precision`).
- Try stronger augmentations (e.g., `RandomHorizontalFlip`, `RandomRotation`, `ColorJitter`).
- Adjust model depth/width depending on dataset difficulty.
- Early stopping and LR schedules can stabilize training.

---


